{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "from pandas_plink import read_plink#1_bin\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn import metrics\n",
    "from math import sqrt\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.environ['CONDA_DEFAULT_ENV'])\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "'''To be changed'''\n",
    "path_for_files=\"/work/biobank/ukb_data/new_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get genotype SNPs data'''\n",
    "(bim,fam,bed)=read_plink(path_for_files+\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get phenotypes (cancer here)'''\n",
    "Yd1=pd.read_csv(path_for_files+\"cancer_pheno.csv\",sep=\",\")\n",
    "Yd2=pd.read_csv(path_for_files+\"non_cancer_pheno.csv\",sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''get intersection of individuals'''\n",
    "ids=list(set(fam[\"iid\"].astype(\"int64\")).intersection(set(Yd1[\"Unnamed: 0\"])))\n",
    "ids.sort()\n",
    "'''Get intersecting individuals in phenotype and concatenate the disease phenotypes with cancer phnotypes to obtain the whole Y'''\n",
    "Yd1=Yd1.set_index('Unnamed: 0').loc[ids].values\n",
    "Yd2=Yd2.set_index('Unnamed: 0').loc[ids].values\n",
    "Yd=np.concatenate([Yd1,Yd2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Reduce size of genotype dataset with convertion in uint8'''\n",
    "print(\"Convertion\")\n",
    "bed=bed.astype('uint8')\n",
    "bed=bed.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''get the matching genotypes wrt phenotypes'''\n",
    "bed=bed[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Divide xarrays for training/validation/test sets'''\n",
    "x_train=bed[:bed.shape[0]*70//100,:]\n",
    "x_val=bed[bed.shape[0]*70//100:bed.shape[0]*85//100,:]\n",
    "x_test=bed[bed.shape[0]*85//100:,:]\n",
    "y_train=Yd[:bed.shape[0]*70//100,:]\n",
    "assert x_train.shape[0]==y_train.shape[0]\n",
    "y_val=Yd[bed.shape[0]*70//100:bed.shape[0]*85//100,:]\n",
    "assert x_val.shape[0]==y_val.shape[0]\n",
    "y_test=Yd[bed.shape[0]*85//100:,:]\n",
    "assert x_test.shape[0]==y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Model'''\n",
    "tf.compat.v1.reset_default_graph()\n",
    "X=tf.placeholder(tf.float32,shape=(None,x_train.shape[1]),name=\"X\")\n",
    "Y=tf.placeholder(tf.float32,shape=(None,y_train.shape[1]),name=\"Y\")\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "    initializer = tf.compat.v1.initializers.he_normal()\n",
    "    hidden00_drop= tf.layers.dropout(X, 0.5, training=training)\n",
    "    hidden01=tf.layers.dense(hidden00_drop, 1000, name=\"hidden01\",activation=None, kernel_initializer=initializer)\n",
    "    hidden01_norm=tf.layers.batch_normalization(hidden01, training=training, momentum=0.9)\n",
    "    act_hidden01=tf.nn.leaky_relu(hidden01_norm)\n",
    "    hidden01_drop = tf.layers.dropout(act_hidden01, 0.5, training=training)\n",
    "    hidden0=tf.layers.dense(hidden01_drop, 200, name=\"hidden0\",activation=None, kernel_initializer=initializer)\n",
    "    hidden0_norm=tf.layers.batch_normalization(hidden0, training=training, momentum=0.9)\n",
    "    act_hidden0=tf.nn.leaky_relu(hidden0_norm)\n",
    "    hidden0_drop = tf.layers.dropout(act_hidden0, 0.5, training=training)\n",
    "    hidden1=tf.layers.dense(hidden0_drop, 50, name=\"hidden1\",activation=None, kernel_initializer=initializer)\n",
    "    hidden1_norm=tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "    act_hidden1=tf.nn.leaky_relu(hidden1_norm)\n",
    "    dp=tf.layers.dropout( act_hidden1, 0.5, training=training)\n",
    "    A=tf.layers.dense(hidden1_norm, x_train.shape[1]+1, name=\"A\",activation=None, kernel_initializer=initializer)\n",
    "    M=tf.multiply(A,X)\n",
    "    M_drop = tf.layers.dropout( M, 0.5, training=training)\n",
    "    weights = tf.trainable_variables()[-2]\n",
    "    output=tf.layers.dense(  M_drop, y_train.shape[1], name=\"output_final\",activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''loss and L2 regularization'''\n",
    "with tf.name_scope(\"loss\"):\n",
    "    l2_regularizer = tf.keras.regularizers.L2(0.001)\n",
    "    cross_entropy =tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=output)\n",
    "    error=tf.reduce_mean(cross_entropy)+l2_regularizer(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adam optimizer'''\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer =tf.train.AdamOptimizer(learning_rate=0.0001,beta1=0.9,beta2=0.999,epsilon=1e-08,use_locking=False,name='Adam')\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        training_op = optimizer.minimize(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''metrics by tensorflow if needed + output activation'''\n",
    "with tf.name_scope(\"eval\"):\n",
    "    predicted = tf.nn.sigmoid(output)\n",
    "    correct_pred = tf.equal(tf.round(predicted), Y)\n",
    "    #acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    #_,auc = tf.metrics.auc(labels=Y[:,1],predictions=predicted[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Reinitialize session if needed'''\n",
    "try:\n",
    "    sess.close()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Initialize training'''\n",
    "init=tf.global_variables_initializer()\n",
    "loc=tf.local_variables_initializer()\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(device_count={ \"GPU\": 1}))\n",
    "init.run()\n",
    "loc.run()\n",
    "accuracy_tab=[]\n",
    "loss_tab=[]\n",
    "epoch_tab=[]\n",
    "accuracy_tab_val=[]\n",
    "loss_tab_val=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Mini Batch function'''\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver=tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Training heuristic'''\n",
    "start = time.time()\n",
    "batch_size=512\n",
    "best_loss=7777\n",
    "size_big_batch=120000\n",
    "size_big_val_test=50000\n",
    "open(path_for_files+'Testc822_mtl_362_l2_dropout.txt', 'w')\n",
    "for epoch in range(200):\n",
    "    iteration=0\n",
    "    '''Perform training iteration'''\n",
    "    for i in range(x_train.shape[0]//size_big_batch+1):\n",
    "        x_train_big=x_train[size_big_batch*i:size_big_batch*(i+1),:].compute()\n",
    "        y_train_big=y_train[size_big_batch*i:size_big_batch*(i+1),:]\n",
    "        for x_batch,y_batch in shuffle_batch(x_train_big,y_train_big, batch_size):#,ones,zeros):\n",
    "            loc.run()\n",
    "            x_batch[np.isnan(x_batch)]=2\n",
    "            sess.run(training_op,feed_dict={X:x_batch,Y:y_batch,W:w,training:True})\n",
    "            print(\"%d ITERATION:%d/%d \"%(epoch,iteration,len(x_train)//batch_size),end='\\r')\n",
    "            iteration+=1\n",
    "        del x_train_big\n",
    "    loc.run()\n",
    "    loss_train=sess.run(error,feed_dict={X:x_batcht,Y:y_batch,W:w,training:False})\n",
    "    print(epoch,\"Batch Train Loss:\",loss_train)\n",
    "    epoch_tab.append(epoch)\n",
    "    loss_tab.append(loss_train)\n",
    "    '''Compute validation scores'''\n",
    "    loss_values=[]\n",
    "    predicted_values=[]\n",
    "    for j in range(x_val.shape[0]//size_big_val_test+1):\n",
    "        loc.run()\n",
    "        x_val_big=x_val[size_big_val_test*j:size_big_val_test*(j+1),:].compute()\n",
    "        x_val_big[np.isnan(x_val_big)]=2\n",
    "        y_val_big=y_val[size_big_val_test*j:size_big_val_test*(j+1),:]\n",
    "        for i in range(x_val_big.shape[0]//batch_size+1):\n",
    "            x_batch_val=x_val_big[i*batch_size:(i+1)*batch_size]\n",
    "            loss_val_loc,predicted_val_loc=sess.run([error,predicted],feed_dict={X:x_batch_val,Y:y_val_big[i*batch_size:(i+1)*batch_size,:],W:w,training:False})\n",
    "            predicted_values.append(predicted_val_loc)\n",
    "            loss_values.append(loss_val_loc)\n",
    "        del x_val_big\n",
    "    loss_val=np.mean(loss_values)\n",
    "    '''Save results if improves best score so far'''\n",
    "    if loss_val<best_loss:\n",
    "        save_path = saver.save(sess, path_for_files+\"modelc822_mtl_362_l2_dropout_loss.ckpt\")\n",
    "        best_loss=loss_val\n",
    "    with open(path_for_files+'Testc822_mtl_362_l2_dropout.txt', 'a+') as file:\n",
    "        file.write(\"Epoch\"+str(epoch)+\" Loss:\"+str(loss_val)+\"\\n\\n\")\n",
    "    print(epoch,\"Loss val:\",loss_val)\n",
    "    print(\"\\n\")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get best model for final test metrics evaluation'''\n",
    "sess.close()\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(device_count={ \"GPU\": 1}))\n",
    "saver.restore(sess, path_for_files+\"modelc822_mtl_362_l2_dropout_loss.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''predict test prediction from the model'''\n",
    "pred_test=[]\n",
    "pred_test=[]\n",
    "size_big_val_test=35000\n",
    "batch_size=512\n",
    "for j in range(x_test.shape[0]//size_big_val_test+1):\n",
    "    loc.run()\n",
    "    x_test_big=x_test[size_big_val_test*j:size_big_val_test*(j+1),:].compute()\n",
    "    x_test_big[np.isnan(x_test_big)]=2\n",
    "    y_test_big=y_test[size_big_val_test*j:size_big_val_test*(j+1),:]\n",
    "    for i in range(x_test_big.shape[0]//batch_size+1):\n",
    "        x_batch_test=x_test_big[i*batch_size:(i+1)*batch_size]\n",
    "        predicted_loc=sess.run(predicted,feed_dict={X:x_batch_test,training:False})\n",
    "        pred_test.append(predicted_loc)\n",
    "    del x_test_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Compute test metrics and store them in a dictionnary disease name--> list of metrics'''\n",
    "from sklearn import metrics\n",
    "pred_test_v=np.concatenate(pred_test,axis=0)\n",
    "diseases=pd.read_csv(path_for_files+\"cancer_pheno.csv\",sep=\",\").drop(columns='Unnamed: 0').columns\n",
    "dic_res={}\n",
    "for i in range(len(y_test.shape[1])):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test[:,i],pred_test_v[:,i], pos_label=1)\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_test[:,i], pred_test_v[:,i], pos_label=1)\n",
    "    dic_res[diseases[i]]=[metrics.auc(fpr, tpr),metrics.auc(recall,precision),metrics.average_precision_score(y_test[:,i],pred_test_v[:,i]),y_test[y_test[:,i]==1].shape[0]/y_test.shape[0]*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Store results'''\n",
    "p=pd.DataFrame(dic_res).T\n",
    "p.columns=[\"AUC ROC test\",\"AUC PR test\",\"Average precision score\",\"prevalence in %\"]\n",
    "p.to_csv(path_for_files+\"Testc822_mtl_362_l2_dropout_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
